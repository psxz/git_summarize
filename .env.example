# ── GitHub ──────────────────────────────────────────────────
GITHUB_TOKEN=ghp_your_personal_access_token

# ── LLM Providers (at least one required) ───────────────────
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=AIza...

# ── Nebius Token Factory (MiniMaxAI/MiniMax-M2.1 and others) ─────────────────
# Get your key at: https://studio.nebius.com/settings/api-keys
# Model catalogue: https://tokenfactory.nebius.com/models
NEBIUS_API_KEY=ey...
# Override base URL only if Nebius adds a new region endpoint
# NEBIUS_API_BASE=https://api.studio.nebius.com/v1/

# ── LLM Configuration ───────────────────────────────────────
DEFAULT_LLM_PROVIDER=openai          # openai | anthropic | google | nebius
DEFAULT_LLM_MODEL=gpt-4o-mini        # model name for the chosen provider
LLM_TEMPERATURE=0.2
LLM_MAX_TOKENS=2048

# ── Summarization Strategy ───────────────────────────────────
SUMMARIZATION_STRATEGY=map_reduce    # map_reduce | refine
CHUNK_SIZE_TOKENS=1500
CHUNK_OVERLAP_TOKENS=150

# ── Cache (optional Redis) ────────────────────────────────────
REDIS_URL=redis://localhost:6379/0
CACHE_TTL_SECONDS=3600

# ── API Security ─────────────────────────────────────────────
# Three auth modes (first matching mode wins):
#
#  1. Logto JWT (production)  — set all LOGTO_* vars below.
#     Users sign in via Logto (GitHub social connector).
#     A JWT-claims script surfaces their GitHub token as a custom claim.
#
#  2. Static key (dev/self-hosted) — set API_SECRET_KEY only.
#     All requests share the service-level GITHUB_TOKEN above.
#
#  3. Open / no auth — leave both unset (local dev only).

# -- Logto (recommended for production) --
LOGTO_ENDPOINT=https://my-tenant.logto.app
LOGTO_API_RESOURCE=https://api.yourapp.com
LOGTO_REQUIRED_SCOPES=["repo:summarize"]
LOGTO_GITHUB_TOKEN_CLAIM=github_access_token

# -- Static key fallback (used when LOGTO_ENDPOINT is not set) --
# API_SECRET_KEY=changeme-use-a-strong-random-key

# ── Observability ────────────────────────────────────────────
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=ls__your_langsmith_key
LANGCHAIN_PROJECT=github-summarizer

# ── Arize Phoenix (self-hosted LLM observability) ────────────
# OTLP/gRPC endpoint — set to enable tracing, leave blank to disable
# Local: http://localhost:4317  |  Docker Compose: http://phoenix:4317
PHOENIX_COLLECTOR_ENDPOINT=http://localhost:4317

# ── Runtime ──────────────────────────────────────────────────
LOG_LEVEL=INFO
MAX_FILES_TO_SCAN=20
MAX_FILE_SIZE_BYTES=102400           # 100 KB hard limit per file
MAX_TOTAL_TOKENS=80000               # guard against insane repos
